<div align="center">
<pre>
     _    _      _         _         ____                      _      _          ____           _     _             
    / \  (_)    / \  _   _| |_ ___  / ___|___  _ __ ___  _ __ | | ___| |_ ___   / ___|__ _  ___| |__ (_)_ __   __ _ 
   / _ \ | |   / _ \| | | | __/ _ \| |   / _ \| '_ ` _ \| '_ \| |/ _ \ __/ _ \ | |   / _` |/ __| '_ \| | '_ \ / _` |
  / ___ \| |  / ___ \ |_| | || (_) | |__| (_) | | | | | | |_) | |  __/ ||  __/ | |__| (_| | (__| | | | | | | | (_| |
 /_/   \_\_| /_/   \_\__,_|\__\___/ \____\___/|_| |_| |_| .__/|_|\___|\__\___|  \____\__,_|\___|_| |_|_|_| |_|\__, |
                                                        |_|                                                   |___/ </pre>
</div>
## Video Demo:
[![Demonstration](https://img.youtube.com/vi/k2XFYcXBXYM/maxresdefault.jpg)](https://youtu.be/k2XFYcXBXYM)
## Description
<hr>![Uploading image.pngâ€¦]()

<!-- Plugin description -->
Ai AutoComplete Caching is an IntelliJ plugin personal project created as a demonstration for a Caching algorithm for an Ai code Autocompletion service, it uses Ollama with the llama 3.2 Ai model to generate code suggestions on every keystroke caching the result in a tree data structure for a quick subsequent retrieval.
<!-- Plugin description end -->

## Features
<hr>

### Ai code suggestions 
Ai suggestions are generated by the llama 3.2 Ai model through Ollama and linked to backend with ollama4j.

### Automatic code Completions 
Code completions are generated automatically by AI or fetched from cache.

### Cache Size
Cache size can be monitored from the plugin control panel which is under Tools -> AutoComplete Caching, it can also be deleted if the button is clicked.

### On/Off Caching
Caching can be turned on/off in order to see the performance increase in the control panel.

### Caching Algorithm
Currently, caching is done with a tree structure where each node has a Character value and an Ai suggestion saved.

### Testing 
Plugin is tested 

## Set Up
<hr>

  - under settings -> plugins -> settings -> install plugin from the disk select the .zip of this plugin 
  - install Ollama (https://ollama.com/download) and go trough the setup process
  - choose a model for Ollama to run for example llama3.2  **"$ollama run llama3.2"**
  - if you have configured another port for Ollama to run on other than the default, change it in the AIModelClient Constructor  
  - By default, caching and autocomplete will be on now
  - Under Tools -> AutoComplete Caching Menu you can find the control panel

## Resources Used
<hr>

  - Ollama (https://ollama.com/download)
  - ollama4j (https://javadoc.io/static/io.github.amithkoujalgi/ollama4j/1.0.60/index.html) for sending and receiving request between the Ollama model and backend
  - IntelliJ's InlineCompletionProvider example of how to use here : (https://github.com/JetBrains/intellij-community/blob/16cc1533d477ac572ac8b8a0c4cf16c10d9069c4/platform/platform-tests/testSrc/com/intellij/codeInsight/inline/completion/impl/InlineCompletionProviders.kt#L84)
