<div align="center">
<pre>
     _    _      _         _         ____                      _      _          ____           _     _             
    / \  (_)    / \  _   _| |_ ___  / ___|___  _ __ ___  _ __ | | ___| |_ ___   / ___|__ _  ___| |__ (_)_ __   __ _ 
   / _ \ | |   / _ \| | | | __/ _ \| |   / _ \| '_ ` _ \| '_ \| |/ _ \ __/ _ \ | |   / _` |/ __| '_ \| | '_ \ / _` |
  / ___ \| |  / ___ \ |_| | || (_) | |__| (_) | | | | | | |_) | |  __/ ||  __/ | |__| (_| | (__| | | | | | | | (_| |
 /_/   \_\_| /_/   \_\__,_|\__\___/ \____\___/|_| |_| |_| .__/|_|\___|\__\___|  \____\__,_|\___|_| |_|_|_| |_|\__, |
                                                        |_|                                                   |___/ </pre>
</div>

[![Demonstration](https://img.youtube.com/vi/3LnUqgzXZ2M/maxresdefault.jpg)](https://youtu.be/3LnUqgzXZ2M)
[//]: # (https://patorjk.com/software/taag/#p=display&f=Ivrit&t=Commi%20tScheduler)
## Description
<hr>
<!-- Plugin description -->
Ai AutoComplete Caching is an IntelliJ plugin personal project created as a demonstration for a Caching algorithm for an Ai code Autocompletion service, it uses Ollama with the llama 3.2 Ai model to generate code suggestions on every keystroke caching the result in a tree data structure for a quick subsequent retrieval.
<!-- Plugin description end -->
## Features
<hr>

### Ai code suggestions 
Ai suggestions are generated by the llama 3.2 Ai model through Ollama and linked to backend with ollama4j.

### Automatic code Completions 
Code completions are generated automatically by AI or fetched from cache.

### See Cache Size
Cache size can be monitored from the plugin control panel which is under Tools -> AutoComplete Caching

### On/Off Caching
Caching can be turned on/off in order to see the performance increase in the control panel.

### Caching Algorithm
Currently, caching is done with a tree structure where each node has a Character value and an Ai suggestion saved.

### Testing 
Plugin is tested 

## Set Up
<hr>

  - under settings -> plugins -> settings -> install plugin from the disk select the .zip of this plugin 
  - install Ollama (https://ollama.com/download) and go trough the setup process
  - choose a model for Ollama to run for example llama3.2  **"$ollama run llama3.2"**
  - if you have configured another port for Ollama to run on other than the default, change it in the AIModelClient Constructor  
  - By default, caching and autocomplete will be on now
  - Under Tools -> AutoComplete Caching Menu you can find the control panel

## Resources Used
<hr>

  - Ollama (https://ollama.com/download)
  - ollama4j (https://javadoc.io/static/io.github.amithkoujalgi/ollama4j/1.0.60/index.html) for sending and receiving request between the Ollama model and backend
  - IntelliJ's InlineCompletionProvider example of how to use here : (https://github.com/JetBrains/intellij-community/blob/16cc1533d477ac572ac8b8a0c4cf16c10d9069c4/platform/platform-tests/testSrc/com/intellij/codeInsight/inline/completion/impl/InlineCompletionProviders.kt#L84)


BUIlDING PROCESS : 
Firstly I imagined a UI for interacting with the plugin, 2 simple actions come to mind
Clearing the cache and turning off the cache to be able to see the performance difference,
for this i created a simple action JBPopup and added it to Tools, then added the buttons to the popup panel
Next, I studied the API for code suggestions and noticed I need to somehow generate the suggestions, Ollama was recommended, so 
I installed **Ollama** from https://ollama.com/download and went through the setup process
i then ran  **"$ollama run llama3.2"** to run the **llama3.2** (chose it because it was one of the smallest size wise just 2GB) LLM model locally, it will generate the code suggestions
Next, we need to link the LLM model to the java backend, this is done through the **ollama4j** client 
firstly add the dependency in build.gradle **"implementation ("com.github.ollama4j:ollama4j:latest_version")"**
* now before to run the plugin we need to make sure to always start the ollama model **"$ollama start"**
Then I created AIModelClient which is a class meant to facilitate communication between the java program and the AI model
Documentation for the fetchChatResult function : https://javadoc.io/static/io.github.amithkoujalgi/ollama4j/1.0.60/index.html
Next, I needed to use the InlineCompletionProvider Interface to make the codeCompletions appear, I couldn t find a documentation, 
so i just copy pasted what I found in the Kotlin example and made a new class tha implements InlineCompletionSuggestion since it needed that
I couldn't implement the Interface in Java because it required a function getId-KYAs() which when I would implement it
the IDE would not recognize it and I just could not implement the interface in java no matter what I did
Next I added getCurrentWord and getSuggestion functionality to the kotlin provider to firstly have the completions be generated by the AIModelClient and secondly
have them be just a continuation of the current word phrase (truncated the suggestion to remove the context)
Currently, when a non word character is inserted the context is refreshed (it also considers \. as a word character)
Would like to add in the future a completionAcceptedListener that triggers another suggestion to be displayed when the current one is accepted
Now to implement the caching, after gathering the context, before asking the AI for a suggestion it will ask the cacheClient
* I don t know how to make a different cacheClient per Project or per file so there will be just one big cache for the whole User


* Currently I have only implemented a simplified version of the Cache, the biggest problem is that I do not have a way of telling
if the user accepted a suggestion (a listener to tab would be bad because tabs are used for other purposes aswell), for this reason,
the whole weight functionality of the cache cannot be used

For the caching algorithm, there are two options, we can hold suggestions in each node, meaning they are very quickly fetched or we can
hold suggestions only in leaf nodes meaning they take longer to fetch and are less relevant because there is one suggestion for a whole tree branch
(we would choose the suggestion of the leaf node itself which could be too specific for early nodes)
Approaches should be chosen depending on memory capabilities of the system, I have implemented the first one.

!! In order for the plugin to automatically use the SimpleInlineCompletionProvide.kt don t forget to put it in plugin.xml

/**
* for context in the 1-3 letters range we will consider it a cache hit no matter what, this first of all saves 26^3 AiCalls
* and second of all there is not enough information in such a short message for the AI to give a useful completion
* in this scenario if we have "print" cached already and the context is "pr" we will return "print" otherwise we will return an empty completion ""
* with usage the cache will have a lot more entries and empty completions will not be common anymore
  */